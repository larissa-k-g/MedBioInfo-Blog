[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Larissa and I am a PhD student at Stockholm University. As part of Prof. Eva Hedlunds research team I concentrate on understanding the motorneuron disease ALS. Most of you probably know it from the “Ice-bucket-challenge” or from the famous physicist Stephen Hawking. On this blog you can follow along if you want to get to know more abut ongoing research in ALS and neuroscience in general. You will also get to see my first attemps to manage different bioinformatic techniques."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "executable code in quarto e.g. using bash needs a python or something environment, otherwise quarto doesn’t now how to execute it. This is a post with executable code."
  },
  {
    "objectID": "posts/Introduction/index.html",
    "href": "posts/Introduction/index.html",
    "title": "How to make a Quarto Blog",
    "section": "",
    "text": "Getting started\n\n\nFirst, create a new folder for a new blog entry. Then create an index.qmd file within that folder. This is going to be where you insert your blog entery. You can add title, author, date and assign a category. There are more options howe Second, add your text and adjust the title etc. In short: make your entry. To check inbetween you can click the preview/rendering button next to the file name or press shift + command + K to get a preview of the updates you made in the webview. To actually safe and publish go into the Terminal again and $git add --all $git commit -m \"Remember to leave a little comment of what the changes that you made were\" $git push origin main In case of this error: “Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing” try: $git pull origin main This should merge the remote updates with your local copy, then try push again. $git push origin main.\nAnd there you go - Your first Quarto Blog Entry!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied-Bioinfo-Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\nggplot for data visualization\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nGetting started with nextflow and nf-core\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nQuality control\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nWork environments and Containers\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nGetting started with a Quarto Blog\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nHarlow Malloc\n\n\nOct 5, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/getting-started-with-quarto-blogs/index.html",
    "href": "posts/getting-started-with-quarto-blogs/index.html",
    "title": "Getting started with a Quarto Blog",
    "section": "",
    "text": "Getting started\n\n\nFirst, create a new folder for a new blog entry. Then create an index.qmd file within that folder. This is going to be where you insert your blog entery. You can add title, author, date and assign a category. There are more options howe Second, add your text and adjust the title etc. In short: make your entry. To check inbetween you can click the preview/rendering button next to the file name or press shift + command + K to get a preview of the updates you made in the webview.\n!Pro Tip: To make paragraphs end the line with two spaces.\n\nFor an indentation try &lt;p style=\"margin-left:2em;\"&gt; Your indented text &lt;/p&gt;\n\n\nTo actually safe and publish go into the Terminal again and\n$git add --all\n$git commit -m \"Remember to leave a little comment of what the changes that you made were\"\n$git push origin main\nIn case of this error: “Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing” try:\n$git pull origin main This should merge the remote updates with your local copy, then try push again.\n$git push origin main.\nAnd there you go - Your first Quarto Blog Entry!"
  },
  {
    "objectID": "posts/pixi-qualitycontrol/index.html",
    "href": "posts/pixi-qualitycontrol/index.html",
    "title": "Work environments and Containers",
    "section": "",
    "text": "!Add slurm explanation!\nIn science reproducibility and accessibility are two key factors. This does not only apply during the data collection phase of a project but particularly also during its analyisis. To ensure that the same script runs the same way anywhere else we can use environments and containers. That sounds great, but what does that mean? Environments are like isolated spaces where you can install all the tools and packages that you need for a specific analysis. It’s basically like a lab bench that you set up with everything you need, chemicals, pipettes etc. for a specific experiment. Environment managers, such as Pixi, make it possible to have multiple different set ups right next to eachother, without one spilling over to the other one and even support you in finding the tools that you want. Containers on the other hand are like a bench on wheels. They contain everything tools, chemicals, hoods, chairs, the bench itself the protocol etc. They are easily shareable and the next person can just open them and run the analysis on their dataset again.\n\nPixi - Environment manager\n\nTo start, install pixie:\n$curl -fsSL https://pixi.sh/install.sh | sh\nCreate a new project directory\nAdd conda-forge and bioconda channels using -c flag:\n$pixi init folder_name -c conda-forge -c bioconda\n\n\n-&gt; Channels will give Pixi a source from were to install the dependencies. It’s like telling Pixi which shop to go to to buy all the tools you need.\n\nIn your new folder you can now find a file - pixi.toml\n\n-&gt; .toml contains all the important information about your environment. It tells you the channels you have added, the name you have given the environment, which operating system, in which version it has been optimized for. Here we can also define different tasks and a list of all the tools/dependencies that have been installed in this environment, after they have been added.\n\nTo add a new tool/dependency you need to be in the project’s folder. Then you can simply use the command:\n$pixi add Quarto\ne.g. to add Quarto\n\n-&gt; to see if it was installed properly you can either test it with the command $pixi run quarto --helpor go back to the .toml file and find that Quarto has been added as a dependency.\n\nNo that you have added a tool, yet another file has been added to your project folder: pixi.lock.\n\n\n-&gt; .lock tracks everything pixi actually did to install your tool, the channels, where the packages were downloaded from, licences etc.\n\n!OBS: DON’T EVER DELETE .toml OR .lock. THIS WILL CAUSE YOUR ENVIRONMENT TO BREAK!\nTo use your tool you can use:\n$pixi run quarto --help\nPro-Tip: You can also enter the environment using $pixi shell. This allows you to forget about pixi and just run your commands as “normally” e.g. `$quarto –help. To leave it again, just type exit into the commandline and press enter.\n\n\n\nContainer images\nTo ensure that your analysis are easily reproducible, indepentent of when and where they are run you can use container images. These are containers that have been created by other people. Here we are using Apptainer as a container, as it is already installed on the course server, however Docker was also recommended. Two common platforms to download container images from are Dockerhub and Seqera.\n\nDockerhub provides the first tool that we want to install: VCFtool, a software created for manipulation and quering of VCF files. It’s like a Swiss Army knife for VCF files - filter, summerize, analyze qenetic/sequencing data. To install it use:\n$apptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n-&gt; apptainer is the software that is called, pull is the command to download/fetch something. “vcftools_0.1.16.1.sif” is the name of the container image. docker:// is te dockerhub registry that we are pulling our software from. “biocontainers/vcftools…” is the user profile that created the container and the name of the container you want to use.\n\nAnother platform for container images is Seqera. This platform is a little different from dockerhub since it does not provide container images that have been uploaded by other users but rather they build them as you request them, by using bioconda, conda-forge and pypi. Thus, the software that you want to use has to be present in one of those reposetories. Change the Container settings on their website from Docker to Singularity and search for bioconda::vcftools=0.1.17.\nClick on get Container, then copy paste the text from the website to the your system:\n$apptainer pull fastqc:0.12.1.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\noras is used, as this is the registry that is being pulled from in this case\n\nHere, we are pulling from the oras registry rather then docker which is why we use oras:// isntead.\nTo run use: $apptainer exec fastqc:0.12.1.sif vcftools --version"
  },
  {
    "objectID": "posts/pixi-qualitycontrol/index.html#pixi---environment-manager",
    "href": "posts/pixi-qualitycontrol/index.html#pixi---environment-manager",
    "title": "Work environments and Containers",
    "section": "Pixi - Environment manager",
    "text": "Pixi - Environment manager\n\nTo start, install pixie run this in your command line:\n$curl -fsSL https://pixi.sh/install.sh | sh\nCreate a new project directory\nAdd conda-forge and bioconda channels using -c flag:\n$pixi init folder_name -c conda-forge -c bioconda\n\n\n-&gt; Channels will give Pixi a source from were to install the dependencies. It’s like telling Pixi which shop to go to to buy all the tools you need.\n\nIn your new folder you can now find a file - pixi.toml\n\n-&gt; .toml contains all the important information about your environment. It tells you the channels you have added, the name you have given the environment, which operating system, in which version it has been optimized for. Here we can also define different tasks and a list of all the tools/dependencies that have been installed in this environment, after they have been added.\n\nTo add a new tool/dependency you need to be in the project’s folder. Then you can simply use the command:\n$pixi add Quarto\ne.g. to add Quarto &lt;p style=\"margin-left:2em;\"&gt; -&gt; to see if it was installed properly you can either test it with the command$pixi run quarto –helpor go back to the .toml file and find that Quarto has been added as a dependency.&lt;/p&gt;\n\nNo that you have added a tool, yet another file has been added to your project folder: pixi.lock.\n&lt;p style=\"margin-left:2em;\"&gt; -&gt; .lock tracks everything pixi actually did to install your tool, the channels, where the packages were downloaded from, licences etc. &lt;/p&gt; !OBS: DON’T EVER DELETE .toml OR .lock. THIS WILL CAUSE YOUR ENVIRONMENT TO BREAK!\nTo use your tool you can use:\n$pixi run quarto --help\" **Pro-Tip:** You can also enter the environment using\\(pixi shell`. This allows you to forget about pixi and just run your commands as \"normally\" e.g. `\\)quarto –help. To leave it again, just type exit into the commandline and press enter."
  },
  {
    "objectID": "posts/nextflow-introduction/index.html",
    "href": "posts/nextflow-introduction/index.html",
    "title": "Getting started with nextflow and nf-core",
    "section": "",
    "text": "In the last Blog we learned that to use tools as e.g. FastQC and MultiQC we need a script with the instructions on how that tool should be used. However, if multiple tools are involved during the progression of a project each tool would need to be started one by one. This is not only very tedious, but also risks inconsitstencies in the workflow and reduces the reproducibility. Therefore, workflow managers are used, to provide pipelines, that allow easier and consistnent workflow and sharing. Nextflow is an open source workflow manager which not only creates an automated pipeline but also allows you to resume from the point of error, if some part of the pipline needs deebugging. It comes with build-in git version control and allows any code language. Generally, workflow managers can run containers as well, which increases the shareability even more. No need to install any softwares and version conflicts are eliminated as well! In the end only one command is needed to execute a whole sequence of scripts and tools, which makes it more accessible to researcher with less bioinformatics experience.\nFundamentally, Nextflow provieds channels, tracking in- and output that connect different modules/tools/processes which are defined in scripts, keeping each process independet. This allows that each script could be written in a differentn scripting language if wanted. Nextflow keeps track, navigates and even allows to parallelize the flow of your data in a time-efficient manner, making it highly scalable as well.\n\nBefore starting\n-&gt; Before getting started set up a screen session, with Pixi as environment manager and add nextflow to your environment:\n$screen -S nextflow\n$pixi init -c bioconda -c conda-forge\n$pixi add nextflow\n\n\nWriting a Nexflow script\nLet’s go step by step through how a .nf file can be set up.\n1. The first line of the script is called a shebang and it defines the interpreter, indicated by #!. Afterwards, the path to the interpreter, within the environment, is defined and nextflow is defined to run the script.\n#!/usr/bin/env nextflow  \n\nIn the next line the parameter “greeting” is defined to carry the value “Hello world!”. This can easily be adjusted or changed if the process should be run on a different string or data set or the like.\n\n\nparams.greeting = 'Hello world!' \n\nIn the next part the channel “greeting_ch” is being initalized. The channel connects the input (so the value of the greeting parameter) to the first process, thus it defines the input of the workflow.\n\n\ngreeting_ch = Channel.of(params.greeting)\n\nNow, the first process block is defined. In this example it is called “Splitletters” and is defined within curly brackets {}. As an input a value is expected which has the variable X assigned to it. However, this could also be a file or path or the like. The output is defined right after. Here, it’s a textfile. All outputs from this process will be called ’chunk_*’, where the asteriks is a wildcard. Next, the script defines a two parted command of printing the value X and then splitting it into groups of 6 characters and to save it one by one in a file e.g. called chunk_ab.\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}  \nRemark: The order of the different parts within the process block does not have any effect on its execution and can be changed to the prefered order, e.g. input, script, output.\n\nIn this next block the process “Converttoupper” follows. It follows the same layout as the previous process block. However, here the input is defined as “path y”, i.e. at least one file is expected. In this example it will thus use the output files generated in “Splitletters”. The output is defined as stdout which stands for standard output. This means it will sent the output to the console or right into the next channel and not create a seperate file. “script:” again defines the command that will be executed. Cat instructs to read the input file and use its content in the next command “tr”, which will convert all lowercase letter to uppercase letters.\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}   \nRemark: The input files are processed parallely, thus the output could be in any order. For this example this would mean that the final output could be “HELLO WORLD!” or “WORLD! HELLO”\n\nSince the order in which the process blocks are defined in the script does not matter when working with nextflow, we define the workflow scope at the end of the script. Here it is defined, which process block is executed (or not) and in which order. Here, the first process to be executed is splitletters, which uses the greeting_ch as its input. The output files will then be emitted into a new channel called letters_ch. The next process “Converttoupper” uses this channel as input. “flatten()” makes sure, that each file will be processed individually, so that the input is not a grouped list. Converttouper will process the files (lower case letters to upper case ones) and send the output into results_ch. Finally, the content of the results channel will be printed directly to the console. “{it}” defines that each element will be displayed immediately.\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n} \n\n\nRunning a nextflow script\nTo run the script use the environment manager that was set up in the beginning:\n$ pixi run nextflow run filename.nf If you go back to the script and change some parts you can use the command:\n$ pixi run nextflow run filename.nf -resume Nextflow will skip all the process that were not modified and use the cached results from the previous run. This allows you to easily test and quickly modify the workflow.\nRemark: Nexflow follows an underlying hierachy, which defines that any parameter specified on the command line will over write the specifications defined in the script. Thus, we can modify e.g. the input string in the execution command to process the phrase “Bonjour le monde!” rather then “Hello World!”:\n$pixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\nCleanup\nAll results that nexflow executed are automatically saved to folder $PWD/work. You can view the execution logs and runtimes from your current. directory with:\n$pixi run nextflow log\nSince this can take up a lot of space you should clean the chached results and the work directory regularly. This can be done manually, or simply be commanding:\n$pixi run nextflow clean\nor also clean all previous runs, before running it again:\n$pixi run nextflow clean -before &lt;RUN NAME&gt; -f\n\n\nnf-core\nnf-core is a community where nexflow modules and piplines a being shared, maintained and developped. It offers a variety of different piplines. However, you should always try to understand if they are relevant to your project and if the process matches what you understand it to be, rather then blindly apply them for your data processing. Understanding them does not only secure that they are suited to be applied in your procesing, but you ca also optimize them to your needs.\nYou can also seek help at the weekly online helpdesk, view training material and get tips and tricks on proper documentation and development.\nEach website on a pipeline is set up with a “usage” tap which describes the piplines function and the input file. Under “Parameters” details to set up the pipline are given. The Output tap can help to interpret the produced output. To add nf-core pipelines to your process start by installing it to your environment, together with nextflow:\n$ pixi add nextflow nf-core\nYou can then run differnt piplines by referencing their location at nf-core:\n$pixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c server.config In this example the Sarek pipeline was used, a test profile was used which uses build-in test data, the results will be safed in a directory called sarek_test. The configuration file’s name and location for the server is defined with option -c."
  },
  {
    "objectID": "posts/Quality-control/index.html",
    "href": "posts/Quality-control/index.html",
    "title": "Quality control",
    "section": "",
    "text": "Determining the quality of your data is key, in order to know if your result is trustworthy or not, should you actually be drawing any conlcusions from your experiment in the first place?\nQuaility control is therefore central, to get to know your data and to understand what you are working with. For sequencing data we can use the software FastQC to check the raw data, before starting a lengthy analysis.\nFastQC is especially valuable if we do high-thoughput sequencing and thus have a lot of data to juggle. A lot of data also means that analysis, might run for several hours, even on a supercomputer. To protect your session from being interupted due to connection issues or because the laptop is accidentally being closed; we can use GNU Screen, a terminal multiplexer. Screen enables to start several independant sessions and even to continue a running process if we disconnect. It basically creates virtual terminals inside our session. Another system we need is a job scheduler. SLURM is such a management system, meaning that it can start and control jobs for you. It is used in computer clusters to organize the queing of different jobs and SLURM also automatically substracts the running time from your project account. So, now that we have our job secured with a screen session and Slurm takes care of the resources we can move on to set the quality control.\nThe best way to keep track of all the instructions is to define them in sequence in a script. To this end, create a new directory for all the scripts within our project folder and create a .sh file for our instructions. Since SLURM is our resoource management we use sbatch to submit a batch script.\nTo do a quality control for your seqeuncing data you can use FastQC. FastQC is a tool, that is suitable for high-throughput sequencing data. It provides information on the general quality on your reads, visualizes the quiality score distribution, as well as the share of each base per sequence. Using FastQC will allow you to identify contaminations due to adapters or other sequences which are overrepresented. It will give you a quality score for an entire sample, instead of single reads. However, the output will still be one file per sample. Therefore, if you need to analyze several samples in one project you can use MultiQC to summerize the output from FastQC.\n\nScreen Session\n\nStart by naming a session\n$screen -S name\nStart your process, then detach from the screen any time and let it run independently. You can then log out or start toher screens and come back to this process at a later time.\nCtrl+ a d\n\nSimply get a list of all open screen session:\n$screen -ls\nThen, reattach to the selected session:\n$screen -r name\n\n\n\nFastQC\n\nUse Pixi to install FastQC or check if it has already been installed:\n\n\n\nto check if it is installed: $ pixi run fastqc --help\nto install:\n$pixi init -c conda-forge -c bioconda\n$pixi add fastqc\n$pixi run fastqc --help\n\n\n\nMultiQC\n\nUse Pixi to install MultiQC or check if it has already been installed:\n\n\n\nto check if it is installed fo into the pixi.toml or run $pixi run multiqc --help\nto install:\n$pixi init -c conda-forge -c bioconda\n$pixi add multiqc\n\n-&gt; To activate the environment run: $pixi shell\n\n\nSlurm\nYou can run Slurm from the command line using:\n$srun -A project_ID -t dd-hh:mm:ss -n 1 &lt;tool options, commands and pathways to the dataset&gt;\n- The project_ID defines the project/account which is charged for the computing time. You can check the projects of which you are a member with $projinfo. This will also allow you to see how many computing hours are left on them.\n- ‘-t’ gives an estimate of the computing time that the job is going to need. This will not only influence the job queueing (shorter run times are preferred) but also defines the max time that will be spent on the task. After the given time the job will be cancelled.\n- to use e.g. FastQC on one or two samples we would define it like this: $srun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data/sample_1.fastq.gz data/sample_2.fastq.gz\n\n\nsbatch\n\nCreate a new folder within your project folder and add a name.sh file for the batch script.\nFor FastQC set your file up like this:\n\n\n#!/bin/bash -l \n#SBATCH -A project_ID \n#SBATCH -t dd-hh:mm:ss\n#SBATCH -n 1 \n\npixi run fastqc -o ../fastqc --noextract ../data/*fastq.gz\n2’. For MultiQC set it up like this\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t dd-hh:mm:ss\n#SBATCH -n 1\n\npixi run multiqc -o ../multiqc ../fastqc\n\nSave the script and run it with pixi:\n$pixi run sbatch name.sh\n\n\n-&gt; you can verify that everything is running properly by checking your job’s status: $ squeue -u &lt;user-name&gt;\nWhen the job is completed you can find the output files in the same folder, where you where in, when you submitted the job. Per default FastQC and MultiQC will give out an .html file which you can download and open locally.\n\n\n\nContainers\nTo use FastQC in containers set up your script like this:\n#! /bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec -B ../data/:/data ../container-image/fastqc:0.12.1.sif fastqc -o ../fastqc-container --noextract ../data/*fastq.gz\nFor more details, on how to get started with containers see my previous post on Work environments and Containers."
  },
  {
    "objectID": "posts/quarto-blogs/index.html",
    "href": "posts/quarto-blogs/index.html",
    "title": "Getting started with a Quarto Blog",
    "section": "",
    "text": "Getting started\n\n\nFirst, create a new folder for a new blog entry. Then create an index.qmd file within that folder. This is going to be where you insert your blog entery. You can add title, author, date and assign a category. There are more options howe Second, add your text and adjust the title etc. In short: make your entry. To check inbetween you can click the preview/rendering button next to the file name or press shift + command + K to get a preview of the updates you made in the webview.\nPro Tip: To make paragraphs end the line with two spaces.\n\nFor an indentation try &lt;p style=\"margin-left:2em;\"&gt; Your indented text &lt;/p&gt;\n\n\nTo actually safe and publish go into the Terminal again and\n$git add --all\n$git commit -m \"Remember to leave a little comment of what the changes that you made were\"\n$git push origin main\nIn case of this error: “Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing” try:\n$git pull origin main This should merge the remote updates with your local copy, then try push again.\n$git push origin main.\nAnd there you go - Your first Quarto Blog Entry!"
  },
  {
    "objectID": "posts/pixi-environment-containers/index.html",
    "href": "posts/pixi-environment-containers/index.html",
    "title": "Work environments and Containers",
    "section": "",
    "text": "In science reproducibility and accessibility are two key factors. This does not only apply during the data collection phase of a project but particularly also during its analyisis. To ensure that the same script runs the same way anywhere else we can use environments and containers. That sounds great, but what does that mean? Environments are like isolated spaces where you can install all the tools and packages that you need for a specific analysis. It’s basically like a lab bench that you set up with everything you need, chemicals, pipettes etc. for a specific experiment. Environment managers, such as Pixi, make it possible to have multiple different set ups right next to eachother, without one spilling over to the other one and even support you in finding the tools that you want. Containers on the other hand are like a bench on wheels. They contain everything tools, chemicals, hoods, chairs, the bench itself the protocol etc. They are easily shareable and the next person can just open them and run the analysis on their dataset again.\n\nPixi - Environment manager\n\nTo start, install pixie:\n$curl -fsSL https://pixi.sh/install.sh | sh\nCreate a new project directory\nAdd conda-forge and bioconda channels using -c flag:\n$pixi init folder_name -c conda-forge -c bioconda\n\n\n-&gt; Channels will give Pixi a source from were to install the dependencies. It’s like telling Pixi which shop to go to to buy all the tools you need.\n\nIn your new folder you can now find a file - pixi.toml\n\n-&gt; .toml contains all the important information about your environment. It tells you the channels you have added, the name you have given the environment, which operating system, in which version it has been optimized for. Here we can also define different tasks and a list of all the tools/dependencies that have been installed in this environment, after they have been added.\n\nTo add a new tool/dependency you need to be in the project’s folder. Then you can simply use the command:\n$pixi add Quarto\ne.g. to add Quarto\n\n-&gt; to see if it was installed properly you can either test it with the command $pixi run quarto --helpor go back to the .toml file and find that Quarto has been added as a dependency.\n\nNo that you have added a tool, yet another file has been added to your project folder: pixi.lock.\n\n\n-&gt; .lock tracks everything pixi actually did to install your tool, the channels, where the packages were downloaded from, licences etc.\n\n!OBS: DON’T EVER DELETE .toml OR .lock. THIS WILL CAUSE YOUR ENVIRONMENT TO BREAK!\nTo use your tool you can use:\n$pixi run quarto --help\nPro-Tip: You can also enter the environment using $pixi shell. This allows you to forget about pixi and just run your commands as “normally” e.g. `$quarto –help. To leave it again, just type exit into the commandline and press enter.\n\n\n\nContainer images\nTo ensure that your analysis are easily reproducible, indepentent of when and where they are run you can use container images. These are containers that have been created by other people. Here we are using Apptainer as a container, as it is already installed on the course server, however Docker was also recommended. Two common platforms to download container images from are Dockerhub and Seqera.\n\nDockerhub provides the first tool that we want to install: VCFtool, a software created for manipulation and quering of VCF files. It’s like a Swiss Army knife for VCF files - filter, summerize, analyze qenetic/sequencing data. To install it use:\n$apptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n-&gt; apptainer is the software that is called, pull is the command to download/fetch something. “vcftools_0.1.16.1.sif” is the name of the container image. docker:// is te dockerhub registry that we are pulling our software from. “biocontainers/vcftools…” is the user profile that created the container and the name of the container you want to use.\n\nAnother platform for container images is Seqera. This platform is a little different from dockerhub since it does not provide container images that have been uploaded by other users but rather they build them as you request them, by using bioconda, conda-forge and pypi. Thus, the software that you want to use has to be present in one of those reposetories. Change the Container settings on their website from Docker to Singularity and search for bioconda::vcftools=0.1.17.\nClick on get Container, then copy paste the text from the website to the your system:\n$apptainer pull fastqc:0.12.1.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\noras is used, as this is the registry that is being pulled from in this case\n\nHere, we are pulling from the oras registry rather then docker which is why we use oras:// isntead.\nTo run use: $apptainer exec fastqc:0.12.1.sif vcftools --version"
  },
  {
    "objectID": "posts/ggplot2/index.html",
    "href": "posts/ggplot2/index.html",
    "title": "ggplot for data visualization",
    "section": "",
    "text": "A central part of the work as a researcher is to present your findings in an approachable and easily understandable way, to your peers, but also to the public. No matter to whom you are going to talk about your research or where you are going to publish it, you will want to visualize your data. And while there are many tools to choice from a R studios provides a very versitile one called “ggplot”. ggplot is like a swiss knive when it comes to generating plots and graphs. Anything from simple dot plots to complex heatmaps and scatterplots can be done using ggplot. To learn the basic syntax and get started with the basics you can follow this tutorial by Roy Francis at the National Bioinformatics Infrastructure Sweden."
  }
]