[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Larissa and I am a PhD student at Stockholm University. As part of Prof. Eva Hedlunds research team I concentrate on understanding the motorneuron disease ALS. Most of you probably know it from the “Ice-bucket-challenge” or from the famous physicist Stephen Hawking. On this blog you can follow along if you want to get to know more abut ongoing research in ALS and neuroscience in general. You will also get to see my first attemps to manage different bioinformatic techniques."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/quarto-blogs/index.html",
    "href": "posts/quarto-blogs/index.html",
    "title": "Getting started with a Quarto Blog",
    "section": "",
    "text": "Getting started\n\n\nFirst, create a new folder for a new blog entry. Then create an index.qmd file within that folder. This is going to be where you insert your blog entery. You can add title, author, date and assign a category. There are more options howe Second, add your text and adjust the title etc. In short: make your entry. To check inbetween you can click the preview/rendering button next to the file name or press shift + command + K to get a preview of the updates you made in the webview.\n!Pro Tip: To make paragraphs end the line with two spaces.\n\nFor an indentation try &lt;p style=\"margin-left:2em;\"&gt; Your indented text &lt;/p&gt;\n\n\nTo actually safe and publish go into the Terminal again and\n$git add --all\n$git commit -m \"Remember to leave a little comment of what the changes that you made were\"\n$git push origin main\nIn case of this error: “Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing” try:\n$git pull origin main This should merge the remote updates with your local copy, then try push again.\n$git push origin main.\nAnd there you go - Your first Quarto Blog Entry!"
  },
  {
    "objectID": "posts/pixi-environment-containers/index.html",
    "href": "posts/pixi-environment-containers/index.html",
    "title": "Work environments and Containers",
    "section": "",
    "text": "In science reproducibility and accessibility are two key factors. This does not only apply during the data collection phase of a project but particularly also during its analyisis. To ensure that the same script runs the same way anywhere else we can use environments and containers. That sounds great, but what does that mean? Environments are like isolated spaces where you can install all the tools and packages that you need for a specific analysis. It’s basically like a lab bench that you set up with everything you need, chemicals, pipettes etc. for a specific experiment. Environment managers, such as Pixi, make it possible to have multiple different set ups right next to eachother, without one spilling over to the other one and even support you in finding the tools that you want. Containers on the other hand are like a bench on wheels. They contain everything tools, chemicals, hoods, chairs, the bench itself the protocol etc. They are easily shareable and the next person can just open them and run the analysis on their dataset again.\n\nPixi - Environment manager\n\nTo start, install pixie:\n$curl -fsSL https://pixi.sh/install.sh | sh\nCreate a new project directory\nAdd conda-forge and bioconda channels using -c flag:\n$pixi init folder_name -c conda-forge -c bioconda\n\n\n-&gt; Channels will give Pixi a source from were to install the dependencies. It’s like telling Pixi which shop to go to to buy all the tools you need.\n\nIn your new folder you can now find a file - pixi.toml\n\n-&gt; .toml contains all the important information about your environment. It tells you the channels you have added, the name you have given the environment, which operating system, in which version it has been optimized for. Here we can also define different tasks and a list of all the tools/dependencies that have been installed in this environment, after they have been added.\n\nTo add a new tool/dependency you need to be in the project’s folder. Then you can simply use the command:\n$pixi add Quarto\ne.g. to add Quarto\n\n-&gt; to see if it was installed properly you can either test it with the command $pixi run quarto --helpor go back to the .toml file and find that Quarto has been added as a dependency.\n\nNo that you have added a tool, yet another file has been added to your project folder: pixi.lock.\n\n\n-&gt; .lock tracks everything pixi actually did to install your tool, the channels, where the packages were downloaded from, licences etc.\n\n!OBS: DON’T EVER DELETE .toml OR .lock. THIS WILL CAUSE YOUR ENVIRONMENT TO BREAK!\nTo use your tool you can use:\n$pixi run quarto --help\nPro-Tip: You can also enter the environment using $pixi shell. This allows you to forget about pixi and just run your commands as “normally” e.g. `$quarto –help. To leave it again, just type exit into the commandline and press enter.\n\n\n\nContainer images\nTo ensure that your analysis are easily reproducible, indepentent of when and where they are run you can use container images. These are containers that have been created by other people. Here we are using Apptainer as a container, as it is already installed on the course server, however Docker was also recommended. Two common platforms to download container images from are Dockerhub and Seqera.\n\nDockerhub provides the first tool that we want to install: VCFtool, a software created for manipulation and quering of VCF files. It’s like a Swiss Army knife for VCF files - filter, summerize, analyze qenetic/sequencing data. To install it use:\n$apptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n-&gt; apptainer is the software that is called, pull is the command to download/fetch something. “vcftools_0.1.16.1.sif” is the name of the container image. docker:// is te dockerhub registry that we are pulling our software from. “biocontainers/vcftools…” is the user profile that created the container and the name of the container you want to use.\n\nAnother platform for container images is Seqera. This platform is a little different from dockerhub since it does not provide container images that have been uploaded by other users but rather they build them as you request them, by using bioconda, conda-forge and pypi. Thus, the software that you want to use has to be present in one of those reposetories. Change the Container settings on their website from Docker to Singularity and search for bioconda::vcftools=0.1.17.\nClick on get Container, then copy paste the text from the website to the your system:\n$apptainer pull fastqc:0.12.1.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\noras is used, as this is the registry that is being pulled from in this case\n\nHere, we are pulling from the oras registry rather then docker which is why we use oras:// isntead.\nTo run use: $apptainer exec fastqc:0.12.1.sif vcftools --version"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied-Bioinfo-Blog",
    "section": "",
    "text": "Getting started with nextflow\n\n\n\nnews\n\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nWork environments and Containers\n\n\n\nnews\n\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nQuality control\n\n\n\nnews\n\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nGetting started with a Quarto Blog\n\n\n\nnews\n\n\n\n\n\n\n\nLarissa Kahnwald\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nOct 3, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "executable code in quarto e.g. using bash needs a python or something environment, otherwise quarto doesn’t now how to execute it. This is a post with executable code."
  },
  {
    "objectID": "posts/nextflow-introduction/index.html",
    "href": "posts/nextflow-introduction/index.html",
    "title": "Getting started with nextflow",
    "section": "",
    "text": "Nextflow creates a pipeline allowing you to resume from the point of error if something happens during your process. It comes with build-in git version control and allows any code language. They can run containers as well, which eliminates the need to install softwares or running into version and environments conflict. On NF you can find different modules to put in your pipeline. Command line has the highest priority, more details to hierachy on canvas pixi run nextflow run hello.nf -resume –greeting ‘Bonjour le monde!’ does then"
  },
  {
    "objectID": "posts/Quality-control/index.html",
    "href": "posts/Quality-control/index.html",
    "title": "Quality control",
    "section": "",
    "text": "Determining the quality of your data is key, in order to know if your result is trustworthy or not, should you actually be drawing any conlcusions from your experiment in the first place?\nQuaility control is therefore central, to get to know your data and to understand what you are working with. For sequencing data we can use the software FastQC to check the raw data, before starting a lengthy analysis.\nFastQC is especially valuable if we do high-thoughput sequencing and thus have a lot of data to juggle. A lot of data also means that analysis, even if automised might run for several hours, even on a supercomputer. To protect our session from being interupted due to connection issues or because the laptop is accidentally being closed; we can use GNU Screen, a terminal multiplexer. Screen enables to start several independant sessions and even to continue a running process if we disconnect. It basically creates virtual terminals inside our session. Another system we need is a job scheduler. SLURM is such a management system, meaning that it can start and control jobs for you. It is used in computer clusters to organize the queing of different jobs and SLURM also automatically substracts the running time from your project account. So, now that we have our job secured with a screen session and Slurm takes care of the resources we can move on to set the quality control.\nThe best way to keep track of all the instructions is to define them in sequence in a script. To this end we create a new directory for all the scripts within our project folder and create a .sh file. Since SLURM is our resoource management we use sbatch to submit our batch script.\n\nScreen Session\n\nStart by naming a session\n$screen -S name\nStart your process, then detach from the screen any time and let it run independently. You can then log out or start toher screens and come back to this process at a later time.\nCtrl+ a d\n\nSimply get a list of all open screen session:\n$screen -ls\nThen, reattach to the selected session:\n$screen -r name\n\n\n\nSlurm\nTo run slurm from the command line you can use:\n$ srun -A project_ID -t dd-hh:mm:ss -n 1 &lt;tool options, commands and pathway to the dataset&gt;\n\n-&gt; the project_ID defines the project/account which is charged for the computing time\n\n\n-&gt; -t gives an estimate of the computing time that the job is going to need. This will not only influence the job queueing (shorter run times are preferred) but also defines the max time that will be spent on the task. After the given time the job will be cancelled.\n\n\nsbatch\n\nCreate a new folder within your project folder and add a .sh file for the batch script.\nSet your header similar to this:\n`$ #! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\nfastqc -o ../fastqc –noextract ../data/*fastq.gz`"
  }
]