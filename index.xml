<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Applied Bioinformatics Blog</title>
<link>https://larissa-k-g.github.io/MedBioInfo-Blog/</link>
<atom:link href="https://larissa-k-g.github.io/MedBioInfo-Blog/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog on getting started with bioinformatics</description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Fri, 10 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>ggplot for data visualization</title>
  <dc:creator>Larissa Kahnwald</dc:creator>
  <link>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/ggplot2/</link>
  <description><![CDATA[ 





<p><img src="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/ggplot2/deng-xiang--WXQm_NTK0U-unsplash.jpg" class="img-fluid" style="float:right; width:45%; margin:0 0 1rem 1rem;"><br>
A central part of the work as a researcher is to present your findings in an approachable and easily understandable way, to your peers, but also to the public. No matter to whom you are going to talk about your research or where you are going to publish it, you will want to visualize your data. And while there are many tools to choose from Rstudio provides a very versitile one called “ggplot”. ggplot is like a swiss army knife when it comes to generating plots and graphs. Anything from simple dot plots to more complex heatmaps, scatterplots or UMAPs can be done using ggplot.</p>
<section id="a-basic-tutorial" class="level4">
<h4 class="anchored" data-anchor-id="a-basic-tutorial">A basic tutorial</h4>
<p>To learn the basic syntax and get started with ggplot you can follow this tutorial by Roy Francis at the National Bioinformatics Infrastructure Sweden.</p>
<iframe src="https://nbisweden.github.io/raukr-2023/labs/ggplot/" width="100%" height="500" style="border: 1px solid #ccc; border-radius: 10px;">
</iframe>


</section>

 ]]></description>
  <category>news</category>
  <guid>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/ggplot2/</guid>
  <pubDate>Fri, 10 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting started with nextflow and nf-core</title>
  <dc:creator>Larissa Kahnwald</dc:creator>
  <link>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/nextflow-introduction/</link>
  <description><![CDATA[ 





<p><img src="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/nextflow-introduction/pipelines.jpg" class="img-fluid" style="float:right; width:45%; margin:0 0 1rem 1rem;"></p>
<p>In the last Blog we learned that to use tools as e.g.&nbsp;<a href="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/Quality-control/#fastqc">FastQC and MultiQC</a> we need a script with the instructions on how that tool should be used. However, if multiple tools are involved during the progression of a project each tool would need to be started one by one. This is not only very tedious, but also risks inconsistencies in the workflow and reduces the reproducibility. Therefore, workflow managers are used to provide pipelines that allow easier and consistnent workflow and sharing. <a href="https://nextflow.io/docs/latest/#nextflow">Nextflow</a> is an open source workflow manager which not only creates an automated pipeline but also allows you to resume from the point of error if some part of the pipline needs deebugging. It comes with built-in git version control and allows any code language. Generally, workflow managers can run containers as well, which increases the shareability even more. No need to install any softwares and version conflicts are eliminated as well! In the end, only one command is needed to execute a whole sequence of scripts and tools, which makes it more accessible to researcher with less bioinformatics experience.<br>
Fundamentally, Nextflow provides channels, tracking in- and output that connect different modules/tools/processes which are defined in scripts, keeping each process independent. Thus, if wanted each script could be written in a different scripting language. Nextflow keeps track, navigates and even allows to parallelize the flow of your data in a time-efficient manner, making it highly scalable as well.</p>
<section id="before-starting" class="level4">
<h4 class="anchored" data-anchor-id="before-starting">Before starting</h4>
<p>-&gt; Before getting started set up a screen session, with Pixi as environment manager and add nextflow to your environment:<br>
<code>$screen -S nextflow</code><br>
<code>$pixi init -c bioconda -c conda-forge</code><br>
<code>$pixi add nextflow</code></p>
</section>
<section id="writing-a-nexflow-script" class="level4">
<h4 class="anchored" data-anchor-id="writing-a-nexflow-script">Writing a Nexflow script</h4>
<p>Let’s go step by step through how a <a href="https://www.nextflow.io/docs/latest/process.html#inputs">.nf file</a> can be set up.<br>
1. The first line of the script is called a shebang and it defines the interpreter, indicated by #!. Afterwards, the path to the interpreter, within the environment, is defined and nextflow is defined to run the script.</p>
<pre class="nextflow"><code>#!/usr/bin/env nextflow  </code></pre>
<ol start="2" type="1">
<li>In the next line the parameter “greeting” is defined to carry the value “Hello world!”. This can easily be adjusted or changed if the process should be run on a different string or data set or the like.<br>
</li>
</ol>
<pre class="nextflow"><code>params.greeting = 'Hello world!' </code></pre>
<ol start="3" type="1">
<li>Next, the channel “greeting_ch” is being initalized. The channel connects the input (so the value of the greeting parameter) to the first process, thus it defines the input of the workflow.<br>
</li>
</ol>
<pre class="nextflow"><code>greeting_ch = Channel.of(params.greeting)</code></pre>
<ol start="4" type="1">
<li>Now, the first process block is defined. In this example it is called “Splitletters” and is defined within curly brackets {}. As an input a value is expected which has the variable X assigned to it. However, this could also be a file or path or the like. The output is defined right after. Here, it’s a textfile. All outputs from this process will be called ’chunk_*’, where the asteriks is a wildcard. Next, the script defines a two parted command of printing the value X and then splitting it into groups of 6 characters and to save it one by one in a file e.g.&nbsp;called chunk_ab.</li>
</ol>
<pre class="nextflow"><code>process SPLITLETTERS {
    input:
    val x

    output:
    path 'chunk_*'

    script:
    """
    printf '$x' | split -b 6 - chunk_
    """
}  </code></pre>
<p><strong>Remark:</strong> The order of the different parts within the process block does not have any effect on its execution and can be changed to the prefered order, e.g.&nbsp;input, script, output.</p>
<ol start="5" type="1">
<li>In this next block the process “Converttoupper” follows. It follows the same layout as the previous process block. However, here the input is defined as “path y”, i.e.&nbsp;at least one file is expected. In this example it will thus use the output files generated in “Splitletters”. The output is defined as stdout which stands for standard output. This means it will sent the output to the console or right into the next channel and not create a seperate file. “script:” again defines the command that will be executed. Cat instructs to read the input file and use its content in the next command “tr”, which will convert all lowercase letter to uppercase letters.</li>
</ol>
<pre class="nextflow"><code>process CONVERTTOUPPER {
    input:
    path y

    output:
    stdout

    script:
    """
    cat $y | tr '[a-z]' '[A-Z]' 
    """
}   </code></pre>
<p><strong>Remark:</strong> The input files are processed parallely, thus the output could be in any order. For this example this would mean that the final output could be “HELLO WORLD!” or “WORLD! HELLO”</p>
<ol start="6" type="1">
<li>Since the order in which the process blocks are defined in the script does not matter when working with nextflow, we define the workflow scope at the end of the script. Here it is defined, which process block is executed (or not) and in which order. Here, the first process to be executed is splitletters, which uses the greeting_ch as its input. The output files will then be emitted into a new channel called letters_ch.&nbsp;The next process “Converttoupper” uses this channel as input. “flatten()” makes sure, that each file will be processed individually, so that the input is not a grouped list. Converttouper will process the files (lower case letters to upper case ones) and send the output into results_ch.&nbsp;Finally, the content of the results channel will be printed directly to the console. “{ it }” defines that each element will be displayed immediately.</li>
</ol>
<pre class="nextflow"><code>workflow {
    letters_ch = SPLITLETTERS(greeting_ch)
    results_ch = CONVERTTOUPPER(letters_ch.flatten())
    results_ch.view{ it }
} </code></pre>
</section>
<section id="running-a-nextflow-script" class="level4">
<h4 class="anchored" data-anchor-id="running-a-nextflow-script">Running a nextflow script</h4>
<p>To run the script use the environment manager that was set up in the beginning:<br>
<code>$ pixi run nextflow run filename.nf</code> If you go back to the script and change some parts you can use the command:<br>
<code>$ pixi run nextflow run filename.nf -resume</code> Nextflow will skip all the process that were not modified and use the cached results from the previous run. This allows you to easily test and quickly modify the workflow.</p>
<p><strong>Remark:</strong> Nexflow follows an underlying hierachy, which defines that any parameter specified on the command line will overwrite the specifications defined in the script. Thus, we can modify e.g.&nbsp;the input string in the execution command to process the phrase “Bonjour le monde!” rather then “Hello World!”:<br>
<code>$pixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'</code></p>
</section>
<section id="cleanup" class="level4">
<h4 class="anchored" data-anchor-id="cleanup">Cleanup</h4>
<p>All results that nexflow executed are automatically saved to folder <code>$PWD/work</code>. You can view the execution logs and runtimes from your current. directory with:<br>
<code>$pixi run nextflow log</code><br>
Since this can take up a lot of space, you should clean the chached results and the work directory regularly. This can be done manually, or simply be commanding:<br>
<code>$pixi run nextflow clean</code><br>
or also clean all previous runs, before running it again:<br>
<code>$pixi run nextflow clean -before &lt;RUN NAME&gt; -f</code></p>
</section>
<section id="nf-core" class="level4">
<h4 class="anchored" data-anchor-id="nf-core">nf-core</h4>
<p>nf-core is a community where nexflow modules and piplines are being shared, maintained and developped. It offers a variety of different piplines. However, you should always try to understand if they are relevant to your project and if the process matches what you understand it to be, rather then blindly apply them for your data processing. Understanding them does not only secure that they are suited to be applied in your procesing, but you can also optimize them to your needs.<br>
You can also seek help at the weekly online helpdesk, view training material and get tips and tricks on proper documentation and development.<br>
Each website on a pipeline is set up with a “usage” tab which describes the piplines function and the input file. Under “Parameters” details to set up the pipline are given. The “output” tab can help to interpret the produced output. To add nf-core pipelines to your process start by installing it to your environment, together with nextflow:<br>
<code>$ pixi add nextflow nf-core</code><br>
You can then run different pipelines by referencing their location at nf-core:<br>
<code>$pixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c server.config</code> In this example the Sarek pipeline was used, a test profile was used which uses build-in test data, the results will be safed in a directory called sarek_test. The configuration file’s name and location for the server is defined with option -c.&nbsp;</p>


</section>

 ]]></description>
  <category>news</category>
  <guid>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/nextflow-introduction/</guid>
  <pubDate>Thu, 09 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Quality control</title>
  <dc:creator>Larissa Kahnwald</dc:creator>
  <link>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/Quality-control/</link>
  <description><![CDATA[ 





<p><img src="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/Quality-control/DNA.jpg" class="img-fluid" style="float:right; width:45%; margin:0 0 1rem 1rem;"></p>
<p>Determining the quality of your data is key, in order to know if your result is trustworthy or not and if you should actually be drawing any conclusions from your experiment in the first place?<br>
Quaility control is therefore central to get to know your data and to understand what you are working with. For sequencing data we can use the software FastQC to check the raw data, before starting a lengthy analysis.<br>
FastQC is especially valuable if we do high-throughput sequencing and thus have a lot of data to juggle. A lot of data also means that the analysis, might run for several hours, even on a supercomputer. To protect your session from being interrupted due to connection issues or because the laptop is accidentally being closed; we can use GNU Screen, a terminal multiplexer. Screen enables to start several independent sessions and even to continue a running process if we disconnect. It basically creates virtual terminals inside our session. Another system we need is a job scheduler. SLURM is such a management system, meaning that it can start and control jobs for you. It is used in computer clusters to organize the queing of different jobs and SLURM also automatically substracts the running time from your project account. So, now that we have our job secured with a screen session and Slurm takes care of the resources we can move on to define the quality control process.<br>
The best way to keep track of all the instructions is to define them in sequence in a script. To this end, create a new directory for all the scripts within our project folder and create a <em>.sh</em> file for our instructions. Since SLURM is our resource management we use sbatch to submit a batch script.<br>
FastQC is a tool that is suitable for high-throughput sequencing data. It provides information on the general quality of your reads, visualizes the quality score distribution, as well as the share of each base per sequence. Using FastQC will allow you to identify contaminations due to adapters or other sequences which are overrepresented. It will give you a quality score for an entire sample instead of single reads. However, the output will still be one file per sample. Therefore, if you need to analyze several samples in one project you can use MultiQC to summerize the output from FastQC.</p>
<section id="screen-session" class="level4">
<h4 class="anchored" data-anchor-id="screen-session">Screen Session</h4>
<ol type="1">
<li>Start by naming a session<br>
<code>$screen -S name</code></li>
<li>Start your process, then detach from the screen any time and let it run independently. You can then log out or start other screens and come back to this process at a later time.<br>
<code>Ctrl+ a d</code><br>
</li>
<li>Simply get a list of all open screen session:<br>
<code>$screen -ls</code><br>
Then, reattach to the selected session:<br>
<code>$screen -r name</code></li>
</ol>
</section>
<section id="fastqc" class="level4">
<h4 class="anchored" data-anchor-id="fastqc">FastQC</h4>
<ol type="1">
<li>Use Pixi to install FastQC or check if it has already been installed:<br>
</li>
</ol>
<ul>
<li>to check if it is installed: <code>$ pixi run fastqc --help</code></li>
<li>to install:<br>
<code>$pixi init -c conda-forge -c bioconda</code><br>
<code>$pixi add fastqc</code></li>
</ul>
</section>
<section id="multiqc" class="level4">
<h4 class="anchored" data-anchor-id="multiqc">MultiQC</h4>
<ol type="1">
<li>Use Pixi to install MultiQC or check if it has already been installed:<br>
</li>
</ol>
<ul>
<li>to check if it is installed fo into the pixi.toml or run <code>$pixi run multiqc --help</code></li>
<li>to install:<br>
<code>$pixi init -c conda-forge -c bioconda</code><br>
<code>$pixi add multiqc</code></li>
</ul>
<p>-&gt; To activate the environment run: <code>$pixi shell</code></p>
</section>
<section id="slurm" class="level4">
<h4 class="anchored" data-anchor-id="slurm">Slurm</h4>
<p>You can run Slurm from the command line using:<br>
<code>$srun -A project_ID -t dd-hh:mm:ss -n 1 &lt;tool options, commands and pathways to the dataset&gt;</code><br>
- The project_ID defines the project/account which is charged for the computing time. You can check the projects of which you are a member with <code>$projinfo</code>. This will also allow you to see how many computing hours are left on them.<br>
- ‘-t’ gives an estimate of the computing time that the job is going to need. This will not only influence the job queueing (shorter run times are preferred) but also defines the max time that will be spent on the task. After the given time the job will be cancelled.<br>
- to use e.g.&nbsp;FastQC on one or two samples we would define it like this: <code>$srun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data/sample_1.fastq.gz data/sample_2.fastq.gz</code></p>
</section>
<section id="sbatch" class="level4">
<h4 class="anchored" data-anchor-id="sbatch">sbatch</h4>
<ol type="1">
<li>Create a new folder within your project folder and add a <code>name.sh</code> file for the batch script.</li>
<li>For FastQC set your file up like this:<br>
</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#!/bin/bash -l </span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -A project_ID </span></span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -t dd-hh:mm:ss</span></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -n 1 </span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pixi</span> run fastqc <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-o</span> ../fastqc <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--noextract</span> ../data/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>fastq.gz</span></code></pre></div></div>
<p>2’. For MultiQC set it up like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#!/bin/bash -l</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -A project_ID</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -t dd-hh:mm:ss</span></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -n 1</span></span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pixi</span> run multiqc <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-o</span> ../multiqc ../fastqc</span></code></pre></div></div>
<ol start="3" type="1">
<li><p>Save the script and run it with pixi:<br>
<code>$pixi run sbatch name.sh</code><br>
</p>
<p style="margin-left:2em;">
</p><p>-&gt; you can verify that everything is running properly by checking your job’s status: <code>$ squeue -u &lt;user-name&gt;</code></p></li>
<li><p>When the job is completed you can find the output files in the same folder where you where in when you submitted the job. Per default FastQC and MultiQC will give out an .html file which you can download and open locally.</p></li>
</ol>
</section>
<section id="containers" class="level4">
<h4 class="anchored" data-anchor-id="containers">Containers</h4>
<p>To use FastQC in containers set up your script like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#! /bin/bash -l</span></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -A hpc2n2025-203</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -t 30:00</span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#SBATCH -n 1</span></span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apptainer</span> exec <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-B</span> ../data/:/data ../container-image/fastqc:0.12.1.sif fastqc <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-o</span> ../fastqc-container <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--noextract</span> ../data/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>fastq.gz</span></code></pre></div></div>
<p>For more details, on how to get started with containers see my previous post on <a href="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/pixi-environment-containers/">Work environments and Containers</a>.</p>


</section>

 ]]></description>
  <category>news</category>
  <guid>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/Quality-control/</guid>
  <pubDate>Wed, 08 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Work environments and Containers</title>
  <dc:creator>Larissa Kahnwald</dc:creator>
  <link>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/pixi-environment-containers/</link>
  <description><![CDATA[ 





<p><img src="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/pixi-environment-containers/pixi2025-10-07.png" class="img-fluid" style="float:right; width:45%; margin:0 0 1rem 1rem;"></p>
<p>In science reproducibility and accessibility are two key factors. This does not only apply during the data collection phase of a project but particularly also during its analysis. To ensure that the same script runs the same way anywhere else, we can use environments and containers. That sounds great, but what does it actually mean? Environments are like isolated spaces where you can install all the tools and packages that you need for a specific analysis. It’s basically like a lab bench that you set up with everything you need, chemicals, pipettes etc. for a specific experiment. Environment managers, such as <strong>Pixi</strong>, make it possible to have multiple different set ups right next to each other, without one spilling over to the other one and even support you in finding the tools that you want. Containers on the other hand are like a bench on wheels. They contain everything: tools, chemicals, hoods, chairs, the bench itself the protocol etc. They are easily shareable and the next person can just open them and run the analysis on their dataset again.</p>
<section id="pixi---environment-manager" class="level4">
<h4 class="anchored" data-anchor-id="pixi---environment-manager">Pixi - Environment manager</h4>
<ol type="1">
<li>To start, install pixi:<br>
<code>$curl -fsSL https://pixi.sh/install.sh | sh</code></li>
<li>Create a new project directory</li>
<li>Add conda-forge and bioconda channels using -c flag:<br>
<code>$pixi init folder_name -c conda-forge -c bioconda</code><br>

<p style="margin-left:2em;">
-&gt; Channels will give Pixi a source from were to install the dependencies. It’s like telling Pixi which shop to go to to buy all the tools you need.
</p></li>
<li>In your new folder you can now find a file - <strong>pixi.toml</strong>
<p style="margin-left:2em;">
-&gt; .toml contains all the important information about your environment. It tells you the channels you have added, the name you have given the environment, which operating system, in which version it has been optimized for. Here we can also define different tasks and a list of all the tools/dependencies that have been installed in this environment, after they have been added.
</p></li>
<li>To add a new tool/dependency you need to be in the project’s folder. Then you can simply use the command:<br>
<code>$pixi add Quarto</code><br>
e.g.&nbsp;to add Quarto
<p style="margin-left:2em;">
-&gt; to see if it was installed properly you can either test it with the command <code>$pixi run quarto --help</code>or go back to the .toml file and verify that Quarto has been added as a dependency.
</p></li>
<li>Now that you have added a tool, yet another file has been added to your project folder: pixi.lock.<br>

<p style="margin-left:2em;">
-&gt; .lock tracks everything pixi actually did to install your tool, the channels, where the packages were downloaded from, licences etc.
</p>
<strong>Note:</strong> DON’T EVER DELETE .toml OR .lock. THIS WILL CAUSE YOUR ENVIRONMENT TO BREAK!</li>
<li>To use your tool you can use:<br>
<code>$pixi run quarto --help</code><br>
<strong>Pro Tip:</strong> You can also enter the environment using <code>$pixi shell</code>. This allows you to forget about pixi and just run your commands “normally” e.g.&nbsp;`$quarto –help. To leave it again, just type exit into the command line and press enter.</li>
</ol>
</section>
<section id="container-images" class="level4">
<h4 class="anchored" data-anchor-id="container-images">Container images</h4>
<p>To ensure that your analysis are easily reproducible, indepentent of when and where they are run you can use container images. These, are containers that have been created by other people. Here, we are using Apptainer as a container, as it is already installed on the course server, however Docker was also recommended. Two common platforms to download container images from are Dockerhub and Seqera.</p>
<ol type="1">
<li>Dockerhub provides the first tool that we want to install: VCFtool, a software created for manipulation and quering of VCF files. It’s like a Swiss Army knife for VCF files - filter, summerize, analyze qenetic/sequencing data. To install it use:<br>
<code>$apptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1</code>
<p style="margin-left:2em;">
-&gt; apptainer is the software that is called, pull is the command to download/fetch something. “vcftools_0.1.16.1.sif” is the name of the container image. docker:// is the dockerhub registry that we are pulling our software from. “biocontainers/vcftools…” is the user profile that created the container and the name of the container you want to use.
</p></li>
<li>Another platform for container images is Seqera. This platform is a little different from dockerhub since it does not provide container images that have been uploaded by other users but rather they build them as you request them, by using bioconda, conda-forge and pypi. Thus, the software that you want to use has to be present in one of those repository. Change the container settings on their website from Docker to Singularity and search for bioconda::vcftools=0.1.17.</li>
<li>Click on get container, then copy paste the text from the website to your system:<br>
<code>$apptainer pull fastqc:0.12.1.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9</code>
<p style="margin-left:2em;">
oras is used, as this is the registry that is being pulled from in this case
</p>
Here, we are pulling from the oras registry rather then docker which is why we use oras:// instead.</li>
<li>To run use: <code>$apptainer exec fastqc:0.12.1.sif vcftools --version</code></li>
</ol>


</section>

 ]]></description>
  <category>news</category>
  <guid>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/pixi-environment-containers/</guid>
  <pubDate>Tue, 07 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting started with a Quarto Blog</title>
  <dc:creator>Larissa Kahnwald</dc:creator>
  <link>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/quarto-blogs/</link>
  <description><![CDATA[ 





<p><img src="https://larissa-k-g.github.io/MedBioInfo-Blog/posts/quarto-blogs/patrick-tomasso-Oaqk7qqNh_c-unsplash.jpg" class="img-fluid" style="float:right; width:45%; margin:0 0 1rem 1rem;"></p>
<p>First, create a new folder for a new blog entry. Then create an index.qmd file within that folder. This is going to be where you insert your blog entery. You can add title, author, date and assign a category. There are more options.<br>
Second, add your text and adjust the title etc. In short: make your entry. To check in between you can click the preview/rendering button next to the file name or press shift + command + K to get a preview of the updates you made in the webview.</p>
<strong>Pro Tip: </strong>To make paragraphs end the line with two spaces.
<p style="margin-left:2em;">
For an indentation try <code>&lt;p style="margin-left:2em;"&gt; Your indented text &lt;/p&gt;</code>
</p>
<hr>
<p>To actually save and publish go into the terminal again and<br>
<code>$git add --all</code><br>
<code>$git commit -m "Remember to leave a little comment of what the changes that you made were"</code><br>
<code>$git push origin main</code><br>
In case of this error: “Updates were rejected because the remote contains work that you do | hint: not have locally. This is usually caused by another repository pushing” try:<br>
<code>$git pull origin main</code> This should merge the remote updates with your local copy, then try push again.<br>
<code>$git push origin main</code>.</p>
<p>And there you go - Your first Quarto Blog Entry!</p>



 ]]></description>
  <category>news</category>
  <guid>https://larissa-k-g.github.io/MedBioInfo-Blog/posts/quarto-blogs/</guid>
  <pubDate>Mon, 06 Oct 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
