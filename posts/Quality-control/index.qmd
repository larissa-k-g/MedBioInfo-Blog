---
title: "Quality control"
author: "Larissa Kahnwald"
date: "2025-10-07"
categories: [news]
---
![](DNA.jpg){style="float:right; width:45%; margin:0 0 1rem 1rem;"}  

Determining the quality of your data is key, in order to know if your result is trustworthy or not, should you actually be drawing any conlcusions from your experiment in the first place?  
Quaility control is therefore central, to get to know your data and to understand what you are working with. For sequencing data we can use the software FastQC to check the raw data, before starting a lengthy analysis.  
FastQC is especially valuable if we do high-thoughput sequencing and thus have a lot of data to juggle. A lot of data also means that analysis, even if automised might run for several hours, even on a supercomputer. To protect our session from being interupted due to connection issues or because the laptop is accidentally being closed; we can use GNU Screen, a terminal multiplexer. Screen enables to start several independant sessions and even to continue a running process if we disconnect. It basically creates virtual terminals inside our session. Another system we need is a job scheduler. SLURM is such a management, meaning that it start and control the jobs. 

#### Screen Session
1. Start by naming a session  
`$screen -S name` 
2. Start your process, then detach from the screen any time and let it run independently. You can then log out or start toher screens and come back to this process at a later time.  
`Ctrl+ a d`  
3. Simply get a list of all open screen session:  
`$screen -ls`  
Then, reattach to the selected session:  
`$ screen -r name`  
   
#### Slurm  
To run slurm from the command line you can use:  
`$srun -A project_ID -t 30:00 -n 1 <tool options and commands>`